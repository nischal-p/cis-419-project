{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nischal-p/cis-419-project/blob/main/Mario_RL_Duel_DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGdfgYQHsGVj"
      },
      "outputs": [],
      "source": [
        "\n",
        "%matplotlib inline\n",
        "!pip install gym-super-mario-bros==7.3.0\n",
        "!sudo apt-get install fceux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdSoXEIisGVl"
      },
      "source": [
        "\n",
        "Train a Mario-playing RL Agent\n",
        "================\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7mPUpuJsGVm"
      },
      "outputs": [],
      "source": [
        "# !pip install gym-super-mario-bros==7.3.0\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a825Ax5esGVn"
      },
      "source": [
        "RL Definitions\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "**Environment** The world that an agent interacts with and learns from.\n",
        "\n",
        "**Action** $a$ : How the Agent responds to the Environment. The\n",
        "set of all possible Actions is called *action-space*.\n",
        "\n",
        "**State** $s$ : The current characteristic of the Environment. The\n",
        "set of all possible States the Environment can be in is called\n",
        "*state-space*.\n",
        "\n",
        "**Reward** $r$ : Reward is the key feedback from Environment to\n",
        "Agent. It is what drives the Agent to learn and to change its future\n",
        "action. An aggregation of rewards over multiple time steps is called\n",
        "**Return**.\n",
        "\n",
        "**Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected\n",
        "return if you start in state $s$, take an arbitrary action\n",
        "$a$, and then for each future time step take the action that\n",
        "maximizes returns. $Q$ can be said to stand for the “quality” of\n",
        "the action in a state. We try to approximate this function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PDvwjA1sGVn"
      },
      "source": [
        "Environment\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "Initialize Environment\n",
        "------------------------\n",
        "\n",
        "In Mario, the environment consists of tubes, mushrooms and other\n",
        "components.\n",
        "\n",
        "When Mario makes an action, the environment responds with the changed\n",
        "(next) state, reward and other info.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br86juUqsGVo",
        "outputId": "05e90abc-f2ec-4284-bf81-fa5bb9b8f461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(240, 256, 3),\n",
            " 0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
          ]
        }
      ],
      "source": [
        "# Initialize Super Mario environment\n",
        "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
        "\n",
        "# Limit the action-space to\n",
        "#   0. walk right\n",
        "#   1. jump right\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgnc0rw5sGVo"
      },
      "source": [
        "Preprocess Environment\n",
        "------------------------\n",
        "\n",
        "Environment data is returned to the agent in ``next_state``. As you saw\n",
        "above, each state is represented by a ``[3, 240, 256]`` size array.\n",
        "Often that is more information than our agent needs; for instance,\n",
        "Mario’s actions do not depend on the color of the pipes or the sky!\n",
        "\n",
        "We use **Wrappers** to preprocess environment data before sending it to\n",
        "the agent.\n",
        "\n",
        "``GrayScaleObservation`` is a common wrapper to transform an RGB image\n",
        "to grayscale; doing so reduces the size of the state representation\n",
        "without losing useful information. Now the size of each state:\n",
        "``[1, 240, 256]``\n",
        "\n",
        "``ResizeObservation`` downsamples each observation into a square image.\n",
        "New size: ``[1, 84, 84]``\n",
        "\n",
        "``SkipFrame`` is a custom wrapper that inherits from ``gym.Wrapper`` and\n",
        "implements the ``step()`` function. Because consecutive frames don’t\n",
        "vary much, we can skip n-intermediate frames without losing much\n",
        "information. The n-th frame aggregates rewards accumulated over each\n",
        "skipped frame.\n",
        "\n",
        "``FrameStack`` is a wrapper that allows us to squash consecutive frames\n",
        "of the environment into a single observation point to feed to our\n",
        "learning model. This way, we can identify if Mario was landing or\n",
        "jumping based on the direction of his movement in the previous several\n",
        "frames.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpwgIkKZsGVp"
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = FrameStack(env, num_stack=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3AJQcbxsGVp"
      },
      "source": [
        "After applying the above wrappers to the environment, the final wrapped\n",
        "state consists of 4 gray-scaled consecutive frames stacked together, as\n",
        "shown above in the image on the left. Each time Mario makes an action,\n",
        "the environment responds with a state of this structure. The structure\n",
        "is represented by a 3-D array of size ``[4, 84, 84]``.\n",
        "\n",
        ".. figure:: /_static/img/mario_env.png\n",
        "   :alt: picture\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVNBvcThsGVq"
      },
      "source": [
        "Agent\n",
        "\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "We create a class ``Mario`` to represent our agent in the game. Mario\n",
        "should be able to:\n",
        "\n",
        "-  **Act** according to the optimal action policy based on the current\n",
        "   state (of the environment).\n",
        "\n",
        "-  **Remember** experiences. Experience = (current state, current\n",
        "   action, reward, next state). Mario *caches* and later *recalls* his\n",
        "   experiences to update his action policy.\n",
        "\n",
        "-  **Learn** a better action policy over time\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBKUMiwosGVq"
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__():\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
        "        pass\n",
        "\n",
        "    def cache(self, experience):\n",
        "        \"\"\"Add the experience to memory\"\"\"\n",
        "        pass\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"Sample experiences from memory\"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19358hPPsGVq"
      },
      "source": [
        "In the following sections, we will populate Mario’s parameters and\n",
        "define his functions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jttz0FPqsGVq"
      },
      "source": [
        "Act\n",
        "--------------\n",
        "\n",
        "For any given state, an agent can choose to do the most optimal action\n",
        "(**exploit**) or a random action (**explore**).\n",
        "\n",
        "Mario randomly explores with a chance of ``self.exploration_rate``; when\n",
        "he chooses to exploit, he relies on ``MarioNet`` (implemented in\n",
        "``Learn`` section) to provide the most optimal action.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmN2SXcisGVr"
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "\n",
        "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        if self.use_cuda:\n",
        "            self.net = self.net.to(device=\"cuda\")\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "    Given a state, choose an epsilon-greedy action and update value of step.\n",
        "\n",
        "    Inputs:\n",
        "    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
        "    Outputs:\n",
        "    action_idx (int): An integer representing which action Mario will perform\n",
        "    \"\"\"\n",
        "        # EXPLORE\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            state = state.__array__()\n",
        "            if self.use_cuda:\n",
        "                state = torch.tensor(state).cuda()\n",
        "            else:\n",
        "                state = torch.tensor(state)\n",
        "            state = state.unsqueeze(0)\n",
        "            print(state.shape)\n",
        "            print(state)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1htdTgGysGVr"
      },
      "source": [
        "Cache and Recall\n",
        "----------------------\n",
        "\n",
        "These two functions serve as Mario’s “memory” process.\n",
        "\n",
        "``cache()``: Each time Mario performs an action, he stores the\n",
        "``experience`` to his memory. His experience includes the current\n",
        "*state*, *action* performed, *reward* from the action, the *next state*,\n",
        "and whether the game is *done*.\n",
        "\n",
        "``recall()``: Mario randomly samples a batch of experiences from his\n",
        "memory, and uses that to learn the game.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgw7gHzVsGVr"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Mario(Mario):  # subclassing for continuity\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.batch_size = 32\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        Inputs:\n",
        "        state (LazyFrame),\n",
        "        next_state (LazyFrame),\n",
        "        action (int),\n",
        "        reward (float),\n",
        "        done(bool))\n",
        "        \"\"\"\n",
        "        state = state.__array__()\n",
        "        next_state = next_state.__array__()\n",
        "\n",
        "        if self.use_cuda:\n",
        "            state = torch.tensor(state).cuda()\n",
        "            next_state = torch.tensor(next_state).cuda()\n",
        "            action = torch.tensor([action]).cuda()\n",
        "            reward = torch.tensor([reward]).cuda()\n",
        "            done = torch.tensor([done]).cuda()\n",
        "        else:\n",
        "            state = torch.tensor(state)\n",
        "            next_state = torch.tensor(next_state)\n",
        "            action = torch.tensor([action])\n",
        "            reward = torch.tensor([reward])\n",
        "            done = torch.tensor([done])\n",
        "\n",
        "        self.memory.append((state, next_state, action, reward, done,))\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn1j1lZnsGVs"
      },
      "source": [
        "Learn\n",
        "--------------\n",
        "\n",
        "Mario uses the `DDQN algorithm <https://arxiv.org/pdf/1509.06461>`__\n",
        "under the hood. DDQN uses two ConvNets - $Q_{online}$ and\n",
        "$Q_{target}$ - that independently approximate the optimal\n",
        "action-value function.\n",
        "\n",
        "In our implementation, we share feature generator ``features`` across\n",
        "$Q_{online}$ and $Q_{target}$, but maintain separate FC\n",
        "classifiers for each. $\\theta_{target}$ (the parameters of\n",
        "$Q_{target}$) is frozen to prevent updation by backprop. Instead,\n",
        "it is periodically synced with $\\theta_{online}$ (more on this\n",
        "later).\n",
        "\n",
        "Neural Network\n",
        "~~~~~~~~~~~~~~~~~~\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EccVDRL0sGVs"
      },
      "outputs": [],
      "source": [
        "class MarioNet(nn.Module):\n",
        "    \"\"\"mini cnn structure\n",
        "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        print(c)\n",
        "        self.fc_input_dim = 3136; #self.conv(autograd.Variable(torch.zeros(1, *input_dim))).view(1, -1).size(1)\n",
        "        self.conv = nn.Sequential(\n",
        "            \n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "        #self.online = nn.Sequential(\n",
        "        #    nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "        #    nn.ReLU(),\n",
        "        #    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "        #    nn.ReLU(),\n",
        "        #    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "        #    nn.ReLU(),\n",
        "        #    nn.Flatten(),\n",
        "        #    nn.Linear(3136, 512),\n",
        "        #    nn.ReLU(),\n",
        "        #    nn.Linear(512, output_dim),\n",
        "        #)\n",
        "\n",
        "        self.tconv = copy.deepcopy(self.conv)\n",
        "        self.tvalue = copy.deepcopy(self.value_stream)\n",
        "        self.tadvantage = copy.deepcopy(self.advantage_stream)\n",
        "\n",
        "\n",
        "        # Q_target parameters are frozen.\n",
        "        for p in self.tconv.parameters():\n",
        "            p.requires_grad = False\n",
        "        for p in self.tvalue.parameters():\n",
        "            p.requires_grad = False\n",
        "        for p in self.tadvantage.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "\n",
        "        if model == \"online\":\n",
        "          features = self.conv(input)\n",
        "          features = features.view(features.size(0), -1)\n",
        "          values = self.value_stream(features)\n",
        "          advantages = self.advantage_stream(features)\n",
        "          qvals = values + (advantages - advantages.mean())\n",
        "\n",
        "        elif model == \"target\":\n",
        "          features = self.tconv(input)\n",
        "          features = features.view(features.size(0), -1)\n",
        "          values = self.tvalue(features)\n",
        "          advantages = self.tadvantage(features)\n",
        "          qvals = values + (advantages - advantages.mean())\n",
        "\n",
        "        return qvals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZrqed6sGVs"
      },
      "source": [
        "TD Estimate & TD Target\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Two values are involved in learning:\n",
        "\n",
        "**TD Estimate** - the predicted optimal $Q^*$ for a given state\n",
        "$s$\n",
        "\n",
        "\\begin{align}{TD}_e = Q_{online}^*(s,a)\\end{align}\n",
        "\n",
        "**TD Target** - aggregation of current reward and the estimated\n",
        "$Q^*$ in the next state $s'$\n",
        "\n",
        "\\begin{align}a' = argmax_{a} Q_{online}(s', a)\\end{align}\n",
        "\n",
        "\\begin{align}{TD}_t = r + \\gamma Q_{target}^*(s',a')\\end{align}\n",
        "\n",
        "Because we don’t know what next action $a'$ will be, we use the\n",
        "action $a'$ maximizes $Q_{online}$ in the next state\n",
        "$s'$.\n",
        "\n",
        "Notice we use the\n",
        "`@torch.no_grad() <https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad>`__\n",
        "decorator on ``td_target()`` to disable gradient calculations here\n",
        "(because we don’t need to backpropagate on $\\theta_{target}$).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d-YiFmbsGVs"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGi_LmFNsGVs"
      },
      "source": [
        "Updating the model\n",
        "~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "As Mario samples inputs from his replay buffer, we compute $TD_t$\n",
        "and $TD_e$ and backpropagate this loss down $Q_{online}$ to\n",
        "update its parameters $\\theta_{online}$ ($\\alpha$ is the\n",
        "learning rate ``lr`` passed to the ``optimizer``)\n",
        "\n",
        "\\begin{align}\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)\\end{align}\n",
        "\n",
        "$\\theta_{target}$ does not update through backpropagation.\n",
        "Instead, we periodically copy $\\theta_{online}$ to\n",
        "$\\theta_{target}$\n",
        "\n",
        "\\begin{align}\\theta_{target} \\leftarrow \\theta_{online}\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCV26s-5sGVs"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.tconv.load_state_dict(self.net.conv.state_dict())\n",
        "        self.net.tvalue.load_state_dict(self.net.value_stream.state_dict())\n",
        "        self.net.tadvantage.load_state_dict(self.net.advantage_stream.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BanDh8zsGVt"
      },
      "source": [
        "Save checkpoint\n",
        "~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0AxJEePsGVt"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyHN343isGVt"
      },
      "source": [
        "Putting it all together\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baUVBaAOsGVt"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.burnin = 1e4  # min. experiences before training\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZdbS_yysGVt"
      },
      "source": [
        "Logging\n",
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfb-YIzEsGVt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1M1wms2sGVt"
      },
      "source": [
        "Let’s play!\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "In this example we run the training loop for 10 episodes, but for Mario to truly learn the ways of\n",
        "his world, we suggest running the loop for at least 40,000 episodes!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tg2UKOrSs-a"
      },
      "source": [
        "https://davidrpugh.github.io/stochastic-expatriate-descent/openai/binder/google-colab/2020/04/16/remote-rendering-gym-envs.html\n",
        "\n",
        "Visualizng the Rendering online"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMu2nuCLQNPP",
        "outputId": "638302b0-ad42-40f7-eac7-7881e36ea391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay==0.2.*) (0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KoyRWqaQjux",
        "outputId": "bc53ddd6-d8e0-4a2e-eb36-cd5d16ada41a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":1005\n"
          ]
        }
      ],
      "source": [
        "import pyvirtualdisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "!echo $DISPLAY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "jgXCppshsGVu",
        "outputId": "123b23c3-a118-486e-ded7-9a5457f2035e"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-80cf60784647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(*objs, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                            else suppress())\n\u001b[1;32m   2099\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2101\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m                     bbox_inches = self.figure.get_tightbbox(renderer,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1736\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2628\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2630\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 626\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_bbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             magnification, unsampled=unsampled)\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    522\u001b[0m                     self, A[..., 3], out_shape, t, alpha=alpha)\n\u001b[1;32m    523\u001b[0m                 output = _resample(  # resample rgb channels\n\u001b[0;32m--> 524\u001b[0;31m                     self, _rgb_to_rgba(A[..., :3]), out_shape, t, alpha=alpha)\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_alpha\u001b[0m  \u001b[0;31m# recombine rgb and alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    200\u001b[0m                     \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mimage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_filternorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                     image_obj.get_filterrad())\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "episodes = 10\n",
        "\n",
        "#fig = plt.figure()\n",
        "#ax = fig.subplots()\n",
        "for e in range(episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "    img = plt.imshow(env.render(mode='rgb_array'))\n",
        "    \n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # Run agent on the state\n",
        "        action = mario.act(state)\n",
        "\n",
        "        #Added\n",
        "        img.set_data(env.render(mode='rgb_array'))\n",
        "        plt.axis('off')\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "        # Agent performs action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Remember\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # Learn\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # Logging\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if e % 20 == 0:\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArYn-_VHsGVu"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGRSjsYTMUI3"
      },
      "outputs": [],
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty1qbbHDbss7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "Mario_RL_Duel_DDQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

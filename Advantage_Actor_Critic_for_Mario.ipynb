{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advantage Actor Critic for Mario.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nischal-p/cis-419-project/blob/main/Advantage_Actor_Critic_for_Mario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kymJzZuo6NOz"
      },
      "source": [
        "# Cartpole learning with Actor Critic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaAGd4N18vXf"
      },
      "source": [
        "## Installing dependancies for the display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJGB5ySZ82s_",
        "outputId": "6beb0418-10e3-442a-da5d-feb7ea444fea"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install gym-super-mario-bros==7.3.0\n",
        "!sudo apt-get install fceux"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym-super-mario-bros==7.3.0\n",
            "  Downloading gym_super_mario_bros-7.3.0-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting nes-py>=8.0.0\n",
            "  Downloading nes_py-8.1.8.tar.gz (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (4.62.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.1.8-cp37-cp37m-linux_x86_64.whl size=432808 sha256=4946aedb6d086b398770270abf03ee507d3b66cc38808ecdcbf96c8f70e21c12\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/05/1f/608f15ab43187096eb5f3087506419c2d9772e97000f3ba025\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.3.0 nes-py-8.1.8\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
            "  libsdl1.2debian\n",
            "Suggested packages:\n",
            "  efp gvfs\n",
            "The following NEW packages will be installed:\n",
            "  fceux libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
            "  libsdl1.2debian\n",
            "0 upgraded, 7 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 2,807 kB of archives.\n",
            "After this operation, 9,112 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-common all 2.24.32-1ubuntu1 [125 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-0 amd64 2.24.32-1ubuntu1 [1,769 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsdl1.2debian amd64 1.2.15+dfsg2-0.1ubuntu0.1 [175 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fceux amd64 2.2.2+dfsg0-1build1 [604 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail18 amd64 2.24.32-1ubuntu1 [14.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail-common amd64 2.24.32-1ubuntu1 [112 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-bin amd64 2.24.32-1ubuntu1 [7,536 B]\n",
            "Fetched 2,807 kB in 1s (2,361 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 7.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libgtk2.0-common_2.24.32-1ubuntu1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../1-libgtk2.0-0_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libsdl1.2debian:amd64.\n",
            "Preparing to unpack .../2-libsdl1.2debian_1.2.15+dfsg2-0.1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsdl1.2debian:amd64 (1.2.15+dfsg2-0.1ubuntu0.1) ...\n",
            "Selecting previously unselected package fceux.\n",
            "Preparing to unpack .../3-fceux_2.2.2+dfsg0-1build1_amd64.deb ...\n",
            "Unpacking fceux (2.2.2+dfsg0-1build1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../4-libgail18_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../5-libgail-common_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../6-libgtk2.0-bin_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Setting up libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Setting up libsdl1.2debian:amd64 (1.2.15+dfsg2-0.1ubuntu0.1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up fceux (2.2.2+dfsg0-1build1) ...\n",
            "update-alternatives: using /usr/games/fceux to provide /usr/bin/nes (nes) in auto mode\n",
            "Setting up libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVEPqChcDGXD",
        "outputId": "3f0367a6-85f9-46ce-9b96-07d2da150d47"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 994 kB of archives.\n",
            "After this operation, 2,981 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 994 kB in 1s (982 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 155422 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\r\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Selecting previously unselected package x11-utils.\r\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\r\n",
            "Unpacking x11-utils (7.7+3build1) ...\r\n",
            "Selecting previously unselected package xvfb.\r\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\r\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\r\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\r\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Setting up x11-utils (7.7+3build1) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "  Downloading PyOpenGL-accelerate-3.1.5.tar.gz (538 kB)\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n",
            "Building wheels for collected packages: PyOpenGL-accelerate\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py): started\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py): finished with status 'done'\n",
            "  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp37-cp37m-linux_x86_64.whl size=1599554 sha256=a50fb32cd78eb6f8bc4407f274d3afc72d2af402048b90bdeac060285ae1a1c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/f5/6f/169afb3f2d476c5e807f8515b3c9bc9b819c3962316aa804eb\n",
            "Successfully built PyOpenGL-accelerate\n",
            "Installing collected packages: EasyProcess, box2d-py, pyvirtualdisplay, PyOpenGL-accelerate\n",
            "Successfully installed EasyProcess-0.3 PyOpenGL-accelerate-3.1.5 box2d-py-2.3.8 pyvirtualdisplay-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGePTIPo6QC-"
      },
      "source": [
        "## Importing Stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8_rmXAG6Fj6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "from itertools import count\n",
        "import os\n",
        "\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfZ2Fd176XO0"
      },
      "source": [
        "## Setting Up stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmtvdYqE9GTI",
        "outputId": "9ac16768-4492-4478-d5fc-82365d02168d"
      },
      "source": [
        "# setting up the display\n",
        "import pyvirtualdisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "!echo $DISPLAY"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":1001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FKt9tPMxFtN",
        "outputId": "3bc3ecab-ed62-4110-a75e-72230cfbdb50"
      },
      "source": [
        "# setting up environment and paramenters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = gym.make(\"SuperMarioBros-1-1-v0\")\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3),\n",
            " 0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB6hXufezq-j"
      },
      "source": [
        "# Pre-process Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D52vpcT7rrbo",
        "outputId": "29ae3a61-5b9c-47f2-999a-6200e75c8330"
      },
      "source": [
        "device"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKhy_dwhzuoV"
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = FrameStack(env, num_stack=4)\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "state_shape = env.observation_space.shape"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGK2LkA96c1w"
      },
      "source": [
        "## The Actor Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqpZzkI6fRY"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        c, h, w = self.input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.linear1 = nn.Linear(3136, 512)\n",
        "        self.linear2 = nn.Linear(512, self.output_dim)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, state):\n",
        "        output = F.relu(self.conv1(state))\n",
        "        output = F.relu(self.conv2(output))\n",
        "        output = F.relu(self.conv3(output))\n",
        "        output = self.flatten(output)\n",
        "        output = F.relu(self.linear1(output))\n",
        "        output = self.linear2(output)\n",
        "        distribution = Categorical(F.softmax(output, dim=-1))\n",
        "        return distribution"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmjoMz706icq"
      },
      "source": [
        "## The Critic Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95a7RbRy6nC8"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        c, h, w = self.input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.linear1 = nn.Linear(3136, 512)\n",
        "        self.linear2 = nn.Linear(512, 1)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, state):\n",
        "      output = F.relu(self.conv1(state))\n",
        "      output = F.relu(self.conv2(output))\n",
        "      output = F.relu(self.conv3(output))\n",
        "      output = self.flatten(output)\n",
        "      output = F.relu(self.linear1(output))\n",
        "      value = self.linear2(output)\n",
        "      return value"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxxjNC0c61Fn"
      },
      "source": [
        "## Compute returns \n",
        "\n",
        "It takes in rewards and computes the return for a certain episode/state using the discount factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9T872Wj7B1Y"
      },
      "source": [
        "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
        "    R = next_value\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        R = rewards[step] + gamma * R * masks[step]\n",
        "        returns.insert(0, R)\n",
        "    return returns\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olWBVGEiQoVs"
      },
      "source": [
        "# Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_1iv9wdQqwT"
      },
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "\n",
        "    def record(self, episode):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_rewards\", \"ep_lengths\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyoh7GaM7Z9x"
      },
      "source": [
        "# Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1frcB9wuOAKc"
      },
      "source": [
        "## Initializing display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2pWd9tTOC6c",
        "outputId": "2eb1c6ae-b679-400e-8be8-f1ff9c96de6d"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "!echo $DISPLAY"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":1005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-25HnJPPON4-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RelYW1H4E6r8",
        "outputId": "9bc7ce13-e36c-4260-80af-825724a5db5f"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7KAROqIK9X-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cae09fd-8552-46da-8b80-452de85fa181"
      },
      "source": [
        "save_dir = Path(\"/content/gdrive/Shareddrives/CIS 519 Project/mario-a2c-checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "actor = Actor(input_dim=(4, 84, 84), output_dim=env.action_space.n).to(device)\n",
        "critic = Critic(input_dim=(4, 84, 84), output_dim=env.action_space.n).to(device)\n",
        "\n",
        "episodes = 40000\n",
        "\n",
        "save_every = 1000\n",
        "log_every = 500\n",
        "\n",
        "optimizerA = optim.Adam(actor.parameters())\n",
        "optimizerC = optim.Adam(critic.parameters())\n",
        "\n",
        "for e in range(1, episodes + 1):\n",
        "    state = env.reset()\n",
        "    log_probs = []\n",
        "    values = []\n",
        "    rewards = []\n",
        "    masks = []\n",
        "    entropy = 0\n",
        "\n",
        "    # img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "    episode_rewards = 0\n",
        "\n",
        "    for i in count():\n",
        "\n",
        "        # Added\n",
        "        # img.set_data(env.render(mode='rgb_array'))\n",
        "        # plt.axis('off')\n",
        "        # display.display(plt.gcf())\n",
        "        # display.clear_output(wait=True)\n",
        "\n",
        "        state = state.__array__()\n",
        "        state = torch.tensor(state).to(device)\n",
        "        state = state.unsqueeze(0)\n",
        "        dist = actor(state)\n",
        "        value = critic(state)\n",
        "\n",
        "        action = dist.sample()\n",
        "\n",
        "        next_state, reward, done, info = env.step(int(action))\n",
        "\n",
        "        logger.log_step(reward)\n",
        "        episode_rewards += reward\n",
        "\n",
        "        log_prob = dist.log_prob(action).unsqueeze(0)\n",
        "        entropy += dist.entropy().mean()\n",
        "\n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
        "        masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done or info[\"flag_get\"]:\n",
        "            print(f'Episode: {e}, Reward: {episode_rewards}')\n",
        "            break\n",
        "    \n",
        "    logger.log_episode()\n",
        "\n",
        "    next_state = next_state.__array__()\n",
        "    next_state = torch.tensor(next_state).to(device)\n",
        "    next_state = next_state.unsqueeze(0)\n",
        "\n",
        "    next_value = critic(next_state)\n",
        "    returns = compute_returns(next_value, rewards, masks)\n",
        "\n",
        "    log_probs = torch.cat(log_probs)\n",
        "    returns = torch.cat(returns).detach()\n",
        "    values = torch.cat(values)\n",
        "\n",
        "    advantage = returns - values\n",
        "\n",
        "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "    critic_loss = advantage.pow(2).mean()\n",
        "\n",
        "    optimizerA.zero_grad()\n",
        "    optimizerC.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    critic_loss.backward()\n",
        "    optimizerA.step()\n",
        "    optimizerC.step()\n",
        "\n",
        "    if e % log_every == 0:\n",
        "        logger.record(episode=e)\n",
        "\n",
        "    if e % save_every == 0:\n",
        "        torch.save(actor, save_dir / f'actor_{e}.pkl')\n",
        "        torch.save(critic, save_dir / f'critic{e}.pkl')\n",
        "# ipythondisplay.clear_output(wait=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Reward: 1280.0\n",
            "Episode: 2, Reward: 626.0\n",
            "Episode: 3, Reward: 231.0\n",
            "Episode: 4, Reward: 1041.0\n",
            "Episode: 5, Reward: 629.0\n",
            "Episode: 6, Reward: 634.0\n",
            "Episode: 7, Reward: 231.0\n",
            "Episode: 8, Reward: 625.0\n",
            "Episode: 9, Reward: 634.0\n",
            "Episode: 10, Reward: 748.0\n",
            "Episode: 11, Reward: 775.0\n",
            "Episode: 12, Reward: 231.0\n",
            "Episode: 13, Reward: 635.0\n",
            "Episode: 14, Reward: 779.0\n",
            "Episode: 15, Reward: 1282.0\n",
            "Episode: 16, Reward: 231.0\n",
            "Episode: 17, Reward: 1024.0\n",
            "Episode: 18, Reward: 611.0\n",
            "Episode: 19, Reward: 231.0\n",
            "Episode: 20, Reward: 593.0\n",
            "Episode: 21, Reward: 627.0\n",
            "Episode: 22, Reward: 711.0\n",
            "Episode: 23, Reward: 217.0\n",
            "Episode: 24, Reward: 732.0\n",
            "Episode: 25, Reward: 615.0\n",
            "Episode: 26, Reward: 628.0\n",
            "Episode: 27, Reward: 1322.0\n",
            "Episode: 28, Reward: 934.0\n",
            "Episode: 29, Reward: 223.0\n",
            "Episode: 30, Reward: 654.0\n",
            "Episode: 31, Reward: 757.0\n",
            "Episode: 32, Reward: 592.0\n",
            "Episode: 33, Reward: 990.0\n",
            "Episode: 34, Reward: 710.0\n",
            "Episode: 35, Reward: 231.0\n",
            "Episode: 36, Reward: 796.0\n",
            "Episode: 37, Reward: 587.0\n",
            "Episode: 38, Reward: 627.0\n",
            "Episode: 39, Reward: 602.0\n",
            "Episode: 40, Reward: 231.0\n",
            "Episode: 41, Reward: 1296.0\n",
            "Episode: 42, Reward: 231.0\n",
            "Episode: 43, Reward: 733.0\n",
            "Episode: 44, Reward: 753.0\n",
            "Episode: 45, Reward: 761.0\n",
            "Episode: 46, Reward: 626.0\n",
            "Episode: 47, Reward: 1003.0\n",
            "Episode: 48, Reward: 1256.0\n",
            "Episode: 49, Reward: 636.0\n",
            "Episode: 50, Reward: 634.0\n",
            "Episode: 51, Reward: 1019.0\n",
            "Episode: 52, Reward: 950.0\n",
            "Episode: 53, Reward: 605.0\n",
            "Episode: 54, Reward: 633.0\n",
            "Episode: 55, Reward: 605.0\n",
            "Episode: 56, Reward: 588.0\n",
            "Episode: 57, Reward: 718.0\n",
            "Episode: 58, Reward: 597.0\n",
            "Episode: 59, Reward: 231.0\n",
            "Episode: 60, Reward: 1034.0\n",
            "Episode: 61, Reward: 231.0\n",
            "Episode: 62, Reward: 667.0\n",
            "Episode: 63, Reward: 1027.0\n",
            "Episode: 64, Reward: 584.0\n",
            "Episode: 65, Reward: 231.0\n",
            "Episode: 66, Reward: 611.0\n",
            "Episode: 67, Reward: 231.0\n",
            "Episode: 68, Reward: 627.0\n",
            "Episode: 69, Reward: 1661.0\n",
            "Episode: 70, Reward: 618.0\n",
            "Episode: 71, Reward: 614.0\n",
            "Episode: 72, Reward: 618.0\n",
            "Episode: 73, Reward: 626.0\n",
            "Episode: 74, Reward: 1032.0\n",
            "Episode: 75, Reward: 1235.0\n",
            "Episode: 76, Reward: 803.0\n",
            "Episode: 77, Reward: 750.0\n",
            "Episode: 78, Reward: 636.0\n",
            "Episode: 79, Reward: 960.0\n",
            "Episode: 80, Reward: 711.0\n",
            "Episode: 81, Reward: 231.0\n",
            "Episode: 82, Reward: 1235.0\n",
            "Episode: 83, Reward: 1027.0\n",
            "Episode: 84, Reward: 732.0\n",
            "Episode: 85, Reward: 618.0\n",
            "Episode: 86, Reward: 578.0\n",
            "Episode: 87, Reward: 231.0\n",
            "Episode: 88, Reward: 693.0\n",
            "Episode: 89, Reward: 215.0\n",
            "Episode: 90, Reward: 986.0\n",
            "Episode: 91, Reward: 622.0\n",
            "Episode: 92, Reward: 1297.0\n",
            "Episode: 93, Reward: 231.0\n",
            "Episode: 94, Reward: 1289.0\n",
            "Episode: 95, Reward: 226.0\n",
            "Episode: 96, Reward: 710.0\n",
            "Episode: 97, Reward: 231.0\n",
            "Episode: 98, Reward: 1032.0\n",
            "Episode: 99, Reward: 231.0\n",
            "Episode: 100, Reward: 222.0\n",
            "Episode: 101, Reward: 231.0\n",
            "Episode: 102, Reward: 1043.0\n",
            "Episode: 103, Reward: 1031.0\n",
            "Episode: 104, Reward: 984.0\n",
            "Episode: 105, Reward: 1234.0\n",
            "Episode: 106, Reward: 602.0\n",
            "Episode: 107, Reward: 592.0\n",
            "Episode: 108, Reward: 1266.0\n",
            "Episode: 109, Reward: 609.0\n",
            "Episode: 110, Reward: 231.0\n",
            "Episode: 111, Reward: 635.0\n",
            "Episode: 112, Reward: 635.0\n",
            "Episode: 113, Reward: 1018.0\n",
            "Episode: 114, Reward: 752.0\n",
            "Episode: 115, Reward: 223.0\n",
            "Episode: 116, Reward: 621.0\n",
            "Episode: 117, Reward: 766.0\n",
            "Episode: 118, Reward: 1027.0\n",
            "Episode: 119, Reward: 1308.0\n",
            "Episode: 120, Reward: 744.0\n",
            "Episode: 121, Reward: 226.0\n",
            "Episode: 122, Reward: 1019.0\n",
            "Episode: 123, Reward: 633.0\n",
            "Episode: 124, Reward: 633.0\n",
            "Episode: 125, Reward: 1035.0\n",
            "Episode: 126, Reward: 614.0\n",
            "Episode: 127, Reward: 231.0\n",
            "Episode: 128, Reward: 1313.0\n",
            "Episode: 129, Reward: 215.0\n",
            "Episode: 130, Reward: 1021.0\n",
            "Episode: 131, Reward: 579.0\n",
            "Episode: 132, Reward: 634.0\n",
            "Episode: 133, Reward: 231.0\n",
            "Episode: 134, Reward: 609.0\n",
            "Episode: 135, Reward: 805.0\n",
            "Episode: 136, Reward: 586.0\n",
            "Episode: 137, Reward: 1210.0\n",
            "Episode: 138, Reward: 231.0\n",
            "Episode: 139, Reward: 231.0\n",
            "Episode: 140, Reward: 635.0\n",
            "Episode: 141, Reward: 231.0\n",
            "Episode: 142, Reward: 605.0\n",
            "Episode: 143, Reward: 231.0\n",
            "Episode: 144, Reward: 231.0\n",
            "Episode: 145, Reward: 635.0\n",
            "Episode: 146, Reward: 231.0\n",
            "Episode: 147, Reward: 1633.0\n",
            "Episode: 148, Reward: 231.0\n",
            "Episode: 149, Reward: 231.0\n",
            "Episode: 150, Reward: 231.0\n",
            "Episode: 151, Reward: 583.0\n",
            "Episode: 152, Reward: 614.0\n",
            "Episode: 153, Reward: 789.0\n",
            "Episode: 154, Reward: 1020.0\n",
            "Episode: 155, Reward: 592.0\n",
            "Episode: 156, Reward: 589.0\n",
            "Episode: 157, Reward: 644.0\n",
            "Episode: 158, Reward: 728.0\n",
            "Episode: 159, Reward: 231.0\n",
            "Episode: 160, Reward: 683.0\n",
            "Episode: 161, Reward: 626.0\n",
            "Episode: 162, Reward: 231.0\n",
            "Episode: 163, Reward: 223.0\n",
            "Episode: 164, Reward: 601.0\n",
            "Episode: 165, Reward: 231.0\n",
            "Episode: 166, Reward: 790.0\n",
            "Episode: 167, Reward: 635.0\n",
            "Episode: 168, Reward: 812.0\n",
            "Episode: 169, Reward: 613.0\n",
            "Episode: 170, Reward: 231.0\n",
            "Episode: 171, Reward: 231.0\n",
            "Episode: 172, Reward: 217.0\n",
            "Episode: 173, Reward: 231.0\n",
            "Episode: 174, Reward: 616.0\n",
            "Episode: 175, Reward: 226.0\n",
            "Episode: 176, Reward: 788.0\n",
            "Episode: 177, Reward: 226.0\n",
            "Episode: 178, Reward: 231.0\n",
            "Episode: 179, Reward: 1299.0\n",
            "Episode: 180, Reward: 738.0\n",
            "Episode: 181, Reward: 763.0\n",
            "Episode: 182, Reward: 636.0\n",
            "Episode: 183, Reward: 634.0\n",
            "Episode: 184, Reward: 1018.0\n",
            "Episode: 185, Reward: 687.0\n",
            "Episode: 186, Reward: 634.0\n",
            "Episode: 187, Reward: 215.0\n",
            "Episode: 188, Reward: 1091.0\n",
            "Episode: 189, Reward: 231.0\n",
            "Episode: 190, Reward: 600.0\n",
            "Episode: 191, Reward: 768.0\n",
            "Episode: 192, Reward: 626.0\n",
            "Episode: 193, Reward: 231.0\n",
            "Episode: 194, Reward: 698.0\n",
            "Episode: 195, Reward: 587.0\n",
            "Episode: 196, Reward: 1016.0\n",
            "Episode: 197, Reward: 215.0\n",
            "Episode: 198, Reward: 1301.0\n",
            "Episode: 199, Reward: 610.0\n",
            "Episode: 200, Reward: 226.0\n",
            "Episode: 201, Reward: 609.0\n",
            "Episode: 202, Reward: 231.0\n",
            "Episode: 203, Reward: 591.0\n",
            "Episode: 204, Reward: 1264.0\n",
            "Episode: 205, Reward: 809.0\n",
            "Episode: 206, Reward: 602.0\n",
            "Episode: 207, Reward: 231.0\n",
            "Episode: 208, Reward: 223.0\n",
            "Episode: 209, Reward: 995.0\n",
            "Episode: 210, Reward: 614.0\n",
            "Episode: 211, Reward: 635.0\n",
            "Episode: 212, Reward: 884.0\n",
            "Episode: 213, Reward: 231.0\n",
            "Episode: 214, Reward: 609.0\n",
            "Episode: 215, Reward: 633.0\n",
            "Episode: 216, Reward: 617.0\n",
            "Episode: 217, Reward: 635.0\n",
            "Episode: 218, Reward: 759.0\n",
            "Episode: 219, Reward: 750.0\n",
            "Episode: 220, Reward: 1246.0\n",
            "Episode: 221, Reward: 763.0\n",
            "Episode: 222, Reward: 231.0\n",
            "Episode: 223, Reward: 605.0\n",
            "Episode: 224, Reward: 629.0\n",
            "Episode: 225, Reward: 223.0\n",
            "Episode: 226, Reward: 1305.0\n",
            "Episode: 227, Reward: 1133.0\n",
            "Episode: 228, Reward: 730.0\n",
            "Episode: 229, Reward: 223.0\n",
            "Episode: 230, Reward: 231.0\n",
            "Episode: 231, Reward: 734.0\n",
            "Episode: 232, Reward: 231.0\n",
            "Episode: 233, Reward: 621.0\n",
            "Episode: 234, Reward: 635.0\n",
            "Episode: 235, Reward: 610.0\n",
            "Episode: 236, Reward: 620.0\n",
            "Episode: 237, Reward: 722.0\n",
            "Episode: 238, Reward: 695.0\n",
            "Episode: 239, Reward: 1013.0\n",
            "Episode: 240, Reward: 1027.0\n",
            "Episode: 241, Reward: 586.0\n",
            "Episode: 242, Reward: 231.0\n",
            "Episode: 243, Reward: 231.0\n",
            "Episode: 244, Reward: 581.0\n",
            "Episode: 245, Reward: 231.0\n",
            "Episode: 246, Reward: 231.0\n",
            "Episode: 247, Reward: 627.0\n",
            "Episode: 248, Reward: 761.0\n",
            "Episode: 249, Reward: 618.0\n",
            "Episode: 250, Reward: 974.0\n",
            "Episode: 251, Reward: 217.0\n",
            "Episode: 252, Reward: 636.0\n",
            "Episode: 253, Reward: 593.0\n",
            "Episode: 254, Reward: 1384.0\n",
            "Episode: 255, Reward: 727.0\n",
            "Episode: 256, Reward: 1314.0\n",
            "Episode: 257, Reward: 803.0\n",
            "Episode: 258, Reward: 231.0\n",
            "Episode: 259, Reward: 231.0\n",
            "Episode: 260, Reward: 752.0\n",
            "Episode: 261, Reward: 1036.0\n",
            "Episode: 262, Reward: 1011.0\n",
            "Episode: 263, Reward: 231.0\n",
            "Episode: 264, Reward: 636.0\n",
            "Episode: 265, Reward: 583.0\n",
            "Episode: 266, Reward: 623.0\n",
            "Episode: 267, Reward: 1020.0\n",
            "Episode: 268, Reward: 1013.0\n",
            "Episode: 269, Reward: 231.0\n",
            "Episode: 270, Reward: 231.0\n",
            "Episode: 271, Reward: 1304.0\n",
            "Episode: 272, Reward: 231.0\n",
            "Episode: 273, Reward: 733.0\n",
            "Episode: 274, Reward: 626.0\n",
            "Episode: 275, Reward: 634.0\n",
            "Episode: 276, Reward: 771.0\n",
            "Episode: 277, Reward: 231.0\n",
            "Episode: 278, Reward: 222.0\n",
            "Episode: 279, Reward: 807.0\n",
            "Episode: 280, Reward: 740.0\n",
            "Episode: 281, Reward: 578.0\n",
            "Episode: 282, Reward: 1034.0\n",
            "Episode: 283, Reward: 1002.0\n",
            "Episode: 284, Reward: 762.0\n",
            "Episode: 285, Reward: 626.0\n",
            "Episode: 286, Reward: 749.0\n",
            "Episode: 287, Reward: 231.0\n",
            "Episode: 288, Reward: 222.0\n",
            "Episode: 289, Reward: 1014.0\n",
            "Episode: 290, Reward: 626.0\n",
            "Episode: 291, Reward: 231.0\n",
            "Episode: 292, Reward: 978.0\n",
            "Episode: 293, Reward: 614.0\n",
            "Episode: 294, Reward: 231.0\n",
            "Episode: 295, Reward: 231.0\n",
            "Episode: 296, Reward: 728.0\n",
            "Episode: 297, Reward: 962.0\n",
            "Episode: 298, Reward: 601.0\n",
            "Episode: 299, Reward: 634.0\n",
            "Episode: 300, Reward: 636.0\n",
            "Episode: 301, Reward: 614.0\n",
            "Episode: 302, Reward: 602.0\n",
            "Episode: 303, Reward: 231.0\n",
            "Episode: 304, Reward: 612.0\n",
            "Episode: 305, Reward: 626.0\n",
            "Episode: 306, Reward: 751.0\n",
            "Episode: 307, Reward: 231.0\n",
            "Episode: 308, Reward: 636.0\n",
            "Episode: 309, Reward: 2961.0\n",
            "Episode: 310, Reward: 734.0\n",
            "Episode: 311, Reward: 223.0\n",
            "Episode: 312, Reward: 808.0\n",
            "Episode: 313, Reward: 1304.0\n",
            "Episode: 314, Reward: 231.0\n",
            "Episode: 315, Reward: 733.0\n",
            "Episode: 316, Reward: 231.0\n",
            "Episode: 317, Reward: 231.0\n",
            "Episode: 318, Reward: 601.0\n",
            "Episode: 319, Reward: 635.0\n",
            "Episode: 320, Reward: 635.0\n",
            "Episode: 321, Reward: 223.0\n",
            "Episode: 322, Reward: 231.0\n",
            "Episode: 323, Reward: 636.0\n",
            "Episode: 324, Reward: 231.0\n",
            "Episode: 325, Reward: 990.0\n",
            "Episode: 326, Reward: 1250.0\n",
            "Episode: 327, Reward: 222.0\n",
            "Episode: 328, Reward: 626.0\n",
            "Episode: 329, Reward: 585.0\n",
            "Episode: 330, Reward: 699.0\n",
            "Episode: 331, Reward: 1534.0\n",
            "Episode: 332, Reward: 1016.0\n",
            "Episode: 333, Reward: 231.0\n",
            "Episode: 334, Reward: 223.0\n",
            "Episode: 335, Reward: 782.0\n",
            "Episode: 336, Reward: 223.0\n",
            "Episode: 337, Reward: 616.0\n",
            "Episode: 338, Reward: 951.0\n",
            "Episode: 339, Reward: 1040.0\n",
            "Episode: 340, Reward: 1273.0\n",
            "Episode: 341, Reward: 967.0\n",
            "Episode: 342, Reward: 1008.0\n",
            "Episode: 343, Reward: 943.0\n",
            "Episode: 344, Reward: 741.0\n",
            "Episode: 345, Reward: 231.0\n",
            "Episode: 346, Reward: 1033.0\n",
            "Episode: 347, Reward: 751.0\n",
            "Episode: 348, Reward: 222.0\n",
            "Episode: 349, Reward: 996.0\n",
            "Episode: 350, Reward: 222.0\n",
            "Episode: 351, Reward: 787.0\n",
            "Episode: 352, Reward: 215.0\n",
            "Episode: 353, Reward: 1309.0\n",
            "Episode: 354, Reward: 634.0\n",
            "Episode: 355, Reward: 1017.0\n",
            "Episode: 356, Reward: 217.0\n",
            "Episode: 357, Reward: 636.0\n",
            "Episode: 358, Reward: 616.0\n",
            "Episode: 359, Reward: 1047.0\n",
            "Episode: 360, Reward: 231.0\n",
            "Episode: 361, Reward: 793.0\n",
            "Episode: 362, Reward: 634.0\n",
            "Episode: 363, Reward: 594.0\n",
            "Episode: 364, Reward: 223.0\n",
            "Episode: 365, Reward: 231.0\n",
            "Episode: 366, Reward: 231.0\n",
            "Episode: 367, Reward: 231.0\n",
            "Episode: 368, Reward: 231.0\n",
            "Episode: 369, Reward: 1037.0\n",
            "Episode: 370, Reward: 627.0\n",
            "Episode: 371, Reward: 633.0\n",
            "Episode: 372, Reward: 627.0\n",
            "Episode: 373, Reward: 811.0\n",
            "Episode: 374, Reward: 218.0\n",
            "Episode: 375, Reward: 1281.0\n",
            "Episode: 376, Reward: 611.0\n",
            "Episode: 377, Reward: 231.0\n",
            "Episode: 378, Reward: 231.0\n",
            "Episode: 379, Reward: 231.0\n",
            "Episode: 380, Reward: 218.0\n",
            "Episode: 381, Reward: 614.0\n",
            "Episode: 382, Reward: 1046.0\n",
            "Episode: 383, Reward: 738.0\n",
            "Episode: 384, Reward: 601.0\n",
            "Episode: 385, Reward: 231.0\n",
            "Episode: 386, Reward: 626.0\n",
            "Episode: 387, Reward: 1028.0\n",
            "Episode: 388, Reward: 711.0\n",
            "Episode: 389, Reward: 1400.0\n",
            "Episode: 390, Reward: 231.0\n",
            "Episode: 391, Reward: 1011.0\n",
            "Episode: 392, Reward: 633.0\n",
            "Episode: 393, Reward: 1020.0\n",
            "Episode: 394, Reward: 1293.0\n",
            "Episode: 395, Reward: 231.0\n",
            "Episode: 396, Reward: 601.0\n",
            "Episode: 397, Reward: 634.0\n",
            "Episode: 398, Reward: 608.0\n",
            "Episode: 399, Reward: 223.0\n",
            "Episode: 400, Reward: 696.0\n",
            "Episode: 401, Reward: 602.0\n",
            "Episode: 402, Reward: 618.0\n",
            "Episode: 403, Reward: 636.0\n",
            "Episode: 404, Reward: 231.0\n",
            "Episode: 405, Reward: 1388.0\n",
            "Episode: 406, Reward: 755.0\n",
            "Episode: 407, Reward: 729.0\n",
            "Episode: 408, Reward: 231.0\n",
            "Episode: 409, Reward: 1264.0\n",
            "Episode: 410, Reward: 731.0\n",
            "Episode: 411, Reward: 231.0\n",
            "Episode: 412, Reward: 596.0\n",
            "Episode: 413, Reward: 1025.0\n",
            "Episode: 414, Reward: 215.0\n",
            "Episode: 415, Reward: 1284.0\n",
            "Episode: 416, Reward: 231.0\n",
            "Episode: 417, Reward: 223.0\n",
            "Episode: 418, Reward: 1403.0\n",
            "Episode: 419, Reward: 1011.0\n",
            "Episode: 420, Reward: 634.0\n",
            "Episode: 421, Reward: 1037.0\n",
            "Episode: 422, Reward: 626.0\n",
            "Episode: 423, Reward: 798.0\n",
            "Episode: 424, Reward: 628.0\n",
            "Episode: 425, Reward: 627.0\n",
            "Episode: 426, Reward: 597.0\n",
            "Episode: 427, Reward: 764.0\n",
            "Episode: 428, Reward: 626.0\n",
            "Episode: 429, Reward: 634.0\n",
            "Episode: 430, Reward: 627.0\n",
            "Episode: 431, Reward: 696.0\n",
            "Episode: 432, Reward: 222.0\n",
            "Episode: 433, Reward: 698.0\n",
            "Episode: 434, Reward: 988.0\n",
            "Episode: 435, Reward: 627.0\n",
            "Episode: 436, Reward: 1293.0\n",
            "Episode: 437, Reward: 614.0\n",
            "Episode: 438, Reward: 1300.0\n",
            "Episode: 439, Reward: 798.0\n",
            "Episode: 440, Reward: 803.0\n",
            "Episode: 441, Reward: 1306.0\n",
            "Episode: 442, Reward: 757.0\n",
            "Episode: 443, Reward: 588.0\n",
            "Episode: 444, Reward: 1292.0\n",
            "Episode: 445, Reward: 218.0\n",
            "Episode: 446, Reward: 1017.0\n",
            "Episode: 447, Reward: 705.0\n",
            "Episode: 448, Reward: 990.0\n",
            "Episode: 449, Reward: 231.0\n",
            "Episode: 450, Reward: 721.0\n",
            "Episode: 451, Reward: 615.0\n",
            "Episode: 452, Reward: 231.0\n",
            "Episode: 453, Reward: 231.0\n",
            "Episode: 454, Reward: 605.0\n",
            "Episode: 455, Reward: 618.0\n",
            "Episode: 456, Reward: 773.0\n",
            "Episode: 457, Reward: 636.0\n",
            "Episode: 458, Reward: 604.0\n",
            "Episode: 459, Reward: 782.0\n",
            "Episode: 460, Reward: 1002.0\n",
            "Episode: 461, Reward: 231.0\n",
            "Episode: 462, Reward: 223.0\n",
            "Episode: 463, Reward: 600.0\n",
            "Episode: 464, Reward: 231.0\n",
            "Episode: 465, Reward: 1014.0\n",
            "Episode: 466, Reward: 614.0\n",
            "Episode: 467, Reward: 622.0\n",
            "Episode: 468, Reward: 231.0\n",
            "Episode: 469, Reward: 747.0\n",
            "Episode: 470, Reward: 635.0\n",
            "Episode: 471, Reward: 636.0\n",
            "Episode: 472, Reward: 606.0\n",
            "Episode: 473, Reward: 1015.0\n",
            "Episode: 474, Reward: 226.0\n",
            "Episode: 475, Reward: 1288.0\n",
            "Episode: 476, Reward: 586.0\n",
            "Episode: 477, Reward: 231.0\n",
            "Episode: 478, Reward: 231.0\n",
            "Episode: 479, Reward: 982.0\n",
            "Episode: 480, Reward: 231.0\n",
            "Episode: 481, Reward: 231.0\n",
            "Episode: 482, Reward: 736.0\n",
            "Episode: 483, Reward: 231.0\n",
            "Episode: 484, Reward: 1020.0\n",
            "Episode: 485, Reward: 594.0\n",
            "Episode: 486, Reward: 620.0\n",
            "Episode: 487, Reward: 765.0\n",
            "Episode: 488, Reward: 628.0\n",
            "Episode: 489, Reward: 231.0\n",
            "Episode: 490, Reward: 231.0\n",
            "Episode: 491, Reward: 1040.0\n",
            "Episode: 492, Reward: 627.0\n",
            "Episode: 493, Reward: 763.0\n",
            "Episode: 494, Reward: 627.0\n",
            "Episode: 495, Reward: 223.0\n",
            "Episode: 496, Reward: 597.0\n",
            "Episode: 497, Reward: 621.0\n",
            "Episode: 498, Reward: 1291.0\n",
            "Episode: 499, Reward: 592.0\n",
            "Episode: 500, Reward: 687.0\n",
            "Episode 500 - Mean Reward 664.98 - Mean Length 188.56 - Time Delta 1596.893 - Time 2021-12-07T18:48:43\n",
            "Episode: 501, Reward: 724.0\n",
            "Episode: 502, Reward: 627.0\n",
            "Episode: 503, Reward: 1303.0\n",
            "Episode: 504, Reward: 1011.0\n",
            "Episode: 505, Reward: 231.0\n",
            "Episode: 506, Reward: 231.0\n",
            "Episode: 507, Reward: 2963.0\n",
            "Episode: 508, Reward: 231.0\n",
            "Episode: 509, Reward: 1156.0\n",
            "Episode: 510, Reward: 666.0\n",
            "Episode: 511, Reward: 231.0\n",
            "Episode: 512, Reward: 1295.0\n",
            "Episode: 513, Reward: 995.0\n",
            "Episode: 514, Reward: 1290.0\n",
            "Episode: 515, Reward: 231.0\n",
            "Episode: 516, Reward: 783.0\n",
            "Episode: 517, Reward: 735.0\n",
            "Episode: 518, Reward: 807.0\n",
            "Episode: 519, Reward: 222.0\n",
            "Episode: 520, Reward: 605.0\n",
            "Episode: 521, Reward: 231.0\n",
            "Episode: 522, Reward: 627.0\n",
            "Episode: 523, Reward: 739.0\n",
            "Episode: 524, Reward: 1224.0\n",
            "Episode: 525, Reward: 635.0\n",
            "Episode: 526, Reward: 601.0\n",
            "Episode: 527, Reward: 231.0\n",
            "Episode: 528, Reward: 635.0\n",
            "Episode: 529, Reward: 231.0\n",
            "Episode: 530, Reward: 231.0\n",
            "Episode: 531, Reward: 785.0\n",
            "Episode: 532, Reward: 736.0\n",
            "Episode: 533, Reward: 628.0\n",
            "Episode: 534, Reward: 231.0\n",
            "Episode: 535, Reward: 584.0\n",
            "Episode: 536, Reward: 1241.0\n",
            "Episode: 537, Reward: 1021.0\n",
            "Episode: 538, Reward: 1033.0\n",
            "Episode: 539, Reward: 231.0\n",
            "Episode: 540, Reward: 618.0\n",
            "Episode: 541, Reward: 614.0\n",
            "Episode: 542, Reward: 1032.0\n",
            "Episode: 543, Reward: 626.0\n",
            "Episode: 544, Reward: 635.0\n",
            "Episode: 545, Reward: 634.0\n",
            "Episode: 546, Reward: 231.0\n",
            "Episode: 547, Reward: 751.0\n",
            "Episode: 548, Reward: 987.0\n",
            "Episode: 549, Reward: 634.0\n",
            "Episode: 550, Reward: 809.0\n",
            "Episode: 551, Reward: 1395.0\n",
            "Episode: 552, Reward: 619.0\n",
            "Episode: 553, Reward: 626.0\n",
            "Episode: 554, Reward: 1024.0\n",
            "Episode: 555, Reward: 231.0\n",
            "Episode: 556, Reward: 996.0\n",
            "Episode: 557, Reward: 1371.0\n",
            "Episode: 558, Reward: 1020.0\n",
            "Episode: 559, Reward: 729.0\n",
            "Episode: 560, Reward: 231.0\n",
            "Episode: 561, Reward: 231.0\n",
            "Episode: 562, Reward: 808.0\n",
            "Episode: 563, Reward: 1299.0\n",
            "Episode: 564, Reward: 581.0\n",
            "Episode: 565, Reward: 749.0\n",
            "Episode: 566, Reward: 231.0\n",
            "Episode: 567, Reward: 1040.0\n",
            "Episode: 568, Reward: 749.0\n",
            "Episode: 569, Reward: 930.0\n",
            "Episode: 570, Reward: 2250.0\n",
            "Episode: 571, Reward: 806.0\n",
            "Episode: 572, Reward: 626.0\n",
            "Episode: 573, Reward: 1338.0\n",
            "Episode: 574, Reward: 215.0\n",
            "Episode: 575, Reward: 717.0\n",
            "Episode: 576, Reward: 1032.0\n",
            "Episode: 577, Reward: 606.0\n",
            "Episode: 578, Reward: 226.0\n",
            "Episode: 579, Reward: 231.0\n",
            "Episode: 580, Reward: 1041.0\n",
            "Episode: 581, Reward: 627.0\n",
            "Episode: 582, Reward: 231.0\n",
            "Episode: 583, Reward: 701.0\n",
            "Episode: 584, Reward: 1006.0\n",
            "Episode: 585, Reward: 231.0\n",
            "Episode: 586, Reward: 1008.0\n",
            "Episode: 587, Reward: 635.0\n",
            "Episode: 588, Reward: 602.0\n",
            "Episode: 589, Reward: 1249.0\n",
            "Episode: 590, Reward: 1269.0\n",
            "Episode: 591, Reward: 582.0\n",
            "Episode: 592, Reward: 1286.0\n",
            "Episode: 593, Reward: 1112.0\n",
            "Episode: 594, Reward: 680.0\n",
            "Episode: 595, Reward: 231.0\n",
            "Episode: 596, Reward: 618.0\n",
            "Episode: 597, Reward: 589.0\n",
            "Episode: 598, Reward: 231.0\n",
            "Episode: 599, Reward: 1308.0\n",
            "Episode: 600, Reward: 610.0\n",
            "Episode: 601, Reward: 700.0\n",
            "Episode: 602, Reward: 231.0\n",
            "Episode: 603, Reward: 760.0\n",
            "Episode: 604, Reward: 231.0\n",
            "Episode: 605, Reward: 1289.0\n",
            "Episode: 606, Reward: 714.0\n",
            "Episode: 607, Reward: 698.0\n",
            "Episode: 608, Reward: 748.0\n",
            "Episode: 609, Reward: 629.0\n",
            "Episode: 610, Reward: 793.0\n",
            "Episode: 611, Reward: 754.0\n",
            "Episode: 612, Reward: 231.0\n",
            "Episode: 613, Reward: 602.0\n",
            "Episode: 614, Reward: 1018.0\n",
            "Episode: 615, Reward: 709.0\n",
            "Episode: 616, Reward: 615.0\n",
            "Episode: 617, Reward: 682.0\n",
            "Episode: 618, Reward: 231.0\n",
            "Episode: 619, Reward: 1315.0\n",
            "Episode: 620, Reward: 634.0\n",
            "Episode: 621, Reward: 231.0\n",
            "Episode: 622, Reward: 634.0\n",
            "Episode: 623, Reward: 1036.0\n",
            "Episode: 624, Reward: 1122.0\n",
            "Episode: 625, Reward: 231.0\n",
            "Episode: 626, Reward: 231.0\n",
            "Episode: 627, Reward: 231.0\n",
            "Episode: 628, Reward: 231.0\n",
            "Episode: 629, Reward: 231.0\n",
            "Episode: 630, Reward: 231.0\n",
            "Episode: 631, Reward: 231.0\n",
            "Episode: 632, Reward: 231.0\n",
            "Episode: 633, Reward: 231.0\n",
            "Episode: 634, Reward: 231.0\n",
            "Episode: 635, Reward: 231.0\n",
            "Episode: 636, Reward: 231.0\n",
            "Episode: 637, Reward: 231.0\n",
            "Episode: 638, Reward: 231.0\n",
            "Episode: 639, Reward: 231.0\n",
            "Episode: 640, Reward: 231.0\n",
            "Episode: 641, Reward: 231.0\n",
            "Episode: 642, Reward: 231.0\n",
            "Episode: 643, Reward: 231.0\n",
            "Episode: 644, Reward: 231.0\n",
            "Episode: 645, Reward: 231.0\n",
            "Episode: 646, Reward: 231.0\n",
            "Episode: 647, Reward: 231.0\n",
            "Episode: 648, Reward: 231.0\n",
            "Episode: 649, Reward: 231.0\n",
            "Episode: 650, Reward: 231.0\n",
            "Episode: 651, Reward: 231.0\n",
            "Episode: 652, Reward: 231.0\n",
            "Episode: 653, Reward: 231.0\n",
            "Episode: 654, Reward: 231.0\n",
            "Episode: 655, Reward: 231.0\n",
            "Episode: 656, Reward: 231.0\n",
            "Episode: 657, Reward: 231.0\n",
            "Episode: 658, Reward: 231.0\n",
            "Episode: 659, Reward: 231.0\n",
            "Episode: 660, Reward: 231.0\n",
            "Episode: 661, Reward: 231.0\n",
            "Episode: 662, Reward: 231.0\n",
            "Episode: 663, Reward: 231.0\n",
            "Episode: 664, Reward: 231.0\n",
            "Episode: 665, Reward: 231.0\n",
            "Episode: 666, Reward: 231.0\n",
            "Episode: 667, Reward: 231.0\n",
            "Episode: 668, Reward: 231.0\n",
            "Episode: 669, Reward: 231.0\n",
            "Episode: 670, Reward: 231.0\n",
            "Episode: 671, Reward: 231.0\n",
            "Episode: 672, Reward: 231.0\n",
            "Episode: 673, Reward: 231.0\n",
            "Episode: 674, Reward: 231.0\n",
            "Episode: 675, Reward: 231.0\n",
            "Episode: 676, Reward: 231.0\n",
            "Episode: 677, Reward: 231.0\n",
            "Episode: 678, Reward: 231.0\n",
            "Episode: 679, Reward: 231.0\n",
            "Episode: 680, Reward: 231.0\n",
            "Episode: 681, Reward: 231.0\n",
            "Episode: 682, Reward: 231.0\n",
            "Episode: 683, Reward: 231.0\n",
            "Episode: 684, Reward: 231.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5bd16f7f243d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ed87044e6721>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# Accumulate reward and repeat the same action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nes_py/wrappers/joypad_space.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# take the step and record the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nes_py/nes_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrollers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;31m# pass the action to the emulator as an unsigned byte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# get the reward for this step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "B160vA6ubqHg",
        "outputId": "f1df7212-ea83-4760-ec44-012e81b58a9e"
      },
      "source": [
        "# for e in range(1):\n",
        "#     state = env.reset()\n",
        "#     img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "#     episode_rewards = 0\n",
        "\n",
        "#     for i in count():\n",
        "\n",
        "#         # Added\n",
        "#         img.set_data(env.render(mode='rgb_array'))\n",
        "#         plt.axis('off')\n",
        "#         display.display(plt.gcf())\n",
        "#         display.clear_output(wait=True)\n",
        "\n",
        "#         state = state.__array__()\n",
        "#         state = torch.tensor(state).to(device)\n",
        "#         state = state.unsqueeze(0)\n",
        "#         dist = actor(state)\n",
        "#         value = critic(state)\n",
        "\n",
        "#         action = dist.sample()\n",
        "\n",
        "#         next_state, reward, done, info = env.step(int(action))\n",
        "\n",
        "#         episode_rewards += reward\n",
        "\n",
        "#         state = next_state\n",
        "\n",
        "#         if done or info[\"flag_get\"]:\n",
        "#             print(f'Episode: {e}, Reward: {episode_rewards}')\n",
        "#             break\n",
        "    \n",
        "#     logger.log_episode()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Reward: 231.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advantage Actor Critic for Mario.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1QKnihasUHT-ODEJtZs9XP_Pwd90MSDV8",
      "authorship_tag": "ABX9TyNLY3Ju+mdMLmpnfT7HyMQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nischal-p/cis-419-project/blob/main/Advantage_Actor_Critic_for_Mario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kymJzZuo6NOz"
      },
      "source": [
        "# Cartpole learning with Actor Critic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaAGd4N18vXf"
      },
      "source": [
        "## Installing dependancies for the display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJGB5ySZ82s_",
        "outputId": "54418584-c4d7-41f6-e6a5-daa0edb88945"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install gym-super-mario-bros==7.3.0\n",
        "!sudo apt-get install fceux"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym-super-mario-bros==7.3.0 in /usr/local/lib/python3.7/dist-packages (7.3.0)\n",
            "Requirement already satisfied: nes-py>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from gym-super-mario-bros==7.3.0) (8.1.8)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (4.62.3)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.0)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fceux is already the newest version (2.2.2+dfsg0-1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVEPqChcDGXD",
        "outputId": "d9530b84-977c-4a5f-e5f4-3ab31006d6bc"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay==0.2.*) (0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGePTIPo6QC-"
      },
      "source": [
        "## Importing Stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8_rmXAG6Fj6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "from itertools import count\n",
        "import os\n",
        "\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfZ2Fd176XO0"
      },
      "source": [
        "## Setting Up stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmtvdYqE9GTI",
        "outputId": "d7f3c25f-242a-4362-dc0d-48cf81271a72"
      },
      "source": [
        "# setting up the display\n",
        "import pyvirtualdisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "!echo $DISPLAY"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":1049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FKt9tPMxFtN",
        "outputId": "cdfc9189-4761-400f-bbbc-c099ae7f225c"
      },
      "source": [
        "# setting up environment and paramenters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = gym.make(\"SuperMarioBros-1-1-v0\")\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3),\n",
            " 0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB6hXufezq-j"
      },
      "source": [
        "# Pre-process Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKhy_dwhzuoV"
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = FrameStack(env, num_stack=4)\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "state_shape = env.observation_space.shape"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGK2LkA96c1w"
      },
      "source": [
        "## The Actor Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqpZzkI6fRY"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        c, h, w = self.input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.linear1 = nn.Linear(3136, 512)\n",
        "        self.linear2 = nn.Linear(512, self.output_dim)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, state):\n",
        "        output = F.relu(self.conv1(state))\n",
        "        output = F.relu(self.conv2(output))\n",
        "        output = F.relu(self.conv3(output))\n",
        "        output = self.flatten(output)\n",
        "        output = F.relu(self.linear1(output))\n",
        "        output = self.linear2(output)\n",
        "        distribution = Categorical(F.softmax(output, dim=-1))\n",
        "        return distribution"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmjoMz706icq"
      },
      "source": [
        "## The Critic Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95a7RbRy6nC8"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        c, h, w = self.input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.linear1 = nn.Linear(3136, 512)\n",
        "        self.linear2 = nn.Linear(512, 1)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, state):\n",
        "      output = F.relu(self.conv1(state))\n",
        "      output = F.relu(self.conv2(output))\n",
        "      output = F.relu(self.conv3(output))\n",
        "      output = self.flatten(output)\n",
        "      output = F.relu(self.linear1(output))\n",
        "      value = self.linear2(output)\n",
        "      return value"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxxjNC0c61Fn"
      },
      "source": [
        "## Compute returns \n",
        "\n",
        "It takes in rewards and computes the return for a certain episode/state using the discount factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9T872Wj7B1Y"
      },
      "source": [
        "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
        "    R = next_value\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        R = rewards[step] + gamma * R * masks[step]\n",
        "        returns.insert(0, R)\n",
        "    return returns\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olWBVGEiQoVs"
      },
      "source": [
        "# Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_1iv9wdQqwT"
      },
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "\n",
        "    def record(self, episode):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_rewards\", \"ep_lengths\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyoh7GaM7Z9x"
      },
      "source": [
        "# Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1frcB9wuOAKc"
      },
      "source": [
        "## Initializing display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2pWd9tTOC6c",
        "outputId": "369f7747-6a33-4c4b-fcc2-de7d702d914a"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "!echo $DISPLAY"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":1053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-25HnJPPON4-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7KAROqIK9X-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17bf18d2-2643-4fa7-9efc-0359e906b113"
      },
      "source": [
        "save_dir = Path(\"mario_a2c_checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "actor = Actor(input_dim=(4, 84, 84), output_dim=env.action_space.n).to(device)\n",
        "critic = Critic(input_dim=(4, 84, 84), output_dim=env.action_space.n).to(device)\n",
        "\n",
        "episodes = 500\n",
        "\n",
        "save_every = 100\n",
        "log_every = 20\n",
        "\n",
        "optimizerA = optim.Adam(actor.parameters(), lr=0.00025)\n",
        "optimizerC = optim.Adam(critic.parameters(), lr=0.00025)\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    log_probs = []\n",
        "    values = []\n",
        "    rewards = []\n",
        "    masks = []\n",
        "    entropy = 0\n",
        "\n",
        "    # img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "    for i in count():\n",
        "\n",
        "        # Added\n",
        "        # img.set_data(env.render(mode='rgb_array'))\n",
        "        # plt.axis('off')\n",
        "        # display.display(plt.gcf())\n",
        "        # display.clear_output(wait=True)\n",
        "\n",
        "        state = state.__array__()\n",
        "        state = torch.tensor(state).to(device)\n",
        "        state = state.unsqueeze(0)\n",
        "        dist = actor(state)\n",
        "        value = critic(state)\n",
        "\n",
        "        action = dist.sample()\n",
        "\n",
        "        next_state, reward, done, info = env.step(int(action))\n",
        "\n",
        "        logger.log_step(reward)\n",
        "\n",
        "        log_prob = dist.log_prob(action).unsqueeze(0)\n",
        "        entropy += dist.entropy().mean()\n",
        "\n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
        "        masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done or info[\"flag_get\"]:\n",
        "            print(f'Episode: {e}, Reward: {reward}')\n",
        "            break\n",
        "    \n",
        "    logger.log_episode()\n",
        "\n",
        "    next_state = next_state.__array__()\n",
        "    next_state = torch.tensor(next_state).to(device)\n",
        "    next_state = next_state.unsqueeze(0)\n",
        "\n",
        "    next_value = critic(next_state)\n",
        "    returns = compute_returns(next_value, rewards, masks)\n",
        "\n",
        "    log_probs = torch.cat(log_probs)\n",
        "    returns = torch.cat(returns).detach()\n",
        "    values = torch.cat(values)\n",
        "\n",
        "    advantage = returns - values\n",
        "\n",
        "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "    critic_loss = advantage.pow(2).mean()\n",
        "\n",
        "    optimizerA.zero_grad()\n",
        "    optimizerC.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    critic_loss.backward()\n",
        "    optimizerA.step()\n",
        "    optimizerC.step()\n",
        "\n",
        "    # if e % log_every == 0:\n",
        "    #     logger.record(episode=e)\n",
        "\n",
        "    if e % save_every == 0:\n",
        "        torch.save(actor, save_dir / f'actor_{e}.pkl')\n",
        "        torch.save(critic, save_dir / f'critic{e}.pkl')\n",
        "# ipythondisplay.clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Reward: -11.0\n",
            "Episode: 1, Reward: -15.0\n",
            "Episode: 2, Reward: -13.0\n",
            "Episode: 3, Reward: -9.0\n",
            "Episode: 4, Reward: -11.0\n",
            "Episode: 5, Reward: -11.0\n",
            "Episode: 6, Reward: -15.0\n",
            "Episode: 7, Reward: -11.0\n",
            "Episode: 8, Reward: -15.0\n",
            "Episode: 9, Reward: -10.0\n",
            "Episode: 10, Reward: -10.0\n",
            "Episode: 11, Reward: -14.0\n",
            "Episode: 12, Reward: -15.0\n",
            "Episode: 13, Reward: -10.0\n",
            "Episode: 14, Reward: -16.0\n",
            "Episode: 15, Reward: -9.0\n",
            "Episode: 16, Reward: -15.0\n",
            "Episode: 17, Reward: -15.0\n",
            "Episode: 18, Reward: -13.0\n",
            "Episode: 19, Reward: -11.0\n",
            "Episode: 20, Reward: -14.0\n",
            "Episode: 21, Reward: -11.0\n",
            "Episode: 22, Reward: -15.0\n",
            "Episode: 23, Reward: -9.0\n",
            "Episode: 24, Reward: -15.0\n",
            "Episode: 25, Reward: -11.0\n",
            "Episode: 26, Reward: -13.0\n",
            "Episode: 27, Reward: -13.0\n",
            "Episode: 28, Reward: -10.0\n",
            "Episode: 29, Reward: -10.0\n",
            "Episode: 30, Reward: -13.0\n",
            "Episode: 31, Reward: -13.0\n",
            "Episode: 32, Reward: -15.0\n",
            "Episode: 33, Reward: -15.0\n",
            "Episode: 34, Reward: -11.0\n",
            "Episode: 35, Reward: -14.0\n",
            "Episode: 36, Reward: -11.0\n",
            "Episode: 37, Reward: -10.0\n",
            "Episode: 38, Reward: -11.0\n",
            "Episode: 39, Reward: -14.0\n",
            "Episode: 40, Reward: -10.0\n",
            "Episode: 41, Reward: -13.0\n",
            "Episode: 42, Reward: -13.0\n",
            "Episode: 43, Reward: -11.0\n",
            "Episode: 44, Reward: -13.0\n",
            "Episode: 45, Reward: -15.0\n",
            "Episode: 46, Reward: -9.0\n",
            "Episode: 47, Reward: -13.0\n",
            "Episode: 48, Reward: -13.0\n",
            "Episode: 49, Reward: -13.0\n",
            "Episode: 50, Reward: -11.0\n",
            "Episode: 51, Reward: -10.0\n",
            "Episode: 52, Reward: -9.0\n",
            "Episode: 53, Reward: -11.0\n",
            "Episode: 54, Reward: -15.0\n",
            "Episode: 55, Reward: -9.0\n",
            "Episode: 56, Reward: -15.0\n",
            "Episode: 57, Reward: -11.0\n",
            "Episode: 58, Reward: -13.0\n",
            "Episode: 59, Reward: -11.0\n",
            "Episode: 60, Reward: -10.0\n",
            "Episode: 61, Reward: -14.0\n",
            "Episode: 62, Reward: -11.0\n",
            "Episode: 63, Reward: -13.0\n",
            "Episode: 64, Reward: -11.0\n",
            "Episode: 65, Reward: -15.0\n",
            "Episode: 66, Reward: -10.0\n",
            "Episode: 67, Reward: -11.0\n",
            "Episode: 68, Reward: -11.0\n",
            "Episode: 69, Reward: -10.0\n",
            "Episode: 70, Reward: -11.0\n",
            "Episode: 71, Reward: -10.0\n",
            "Episode: 72, Reward: -12.0\n",
            "Episode: 73, Reward: -11.0\n",
            "Episode: 74, Reward: -11.0\n",
            "Episode: 75, Reward: -10.0\n",
            "Episode: 76, Reward: -11.0\n",
            "Episode: 77, Reward: -11.0\n",
            "Episode: 78, Reward: -10.0\n",
            "Episode: 79, Reward: -10.0\n",
            "Episode: 80, Reward: -11.0\n",
            "Episode: 81, Reward: -14.0\n",
            "Episode: 82, Reward: -11.0\n",
            "Episode: 83, Reward: -14.0\n",
            "Episode: 84, Reward: -11.0\n",
            "Episode: 85, Reward: -11.0\n",
            "Episode: 86, Reward: -14.0\n",
            "Episode: 87, Reward: -15.0\n",
            "Episode: 88, Reward: -10.0\n",
            "Episode: 89, Reward: -15.0\n",
            "Episode: 90, Reward: -11.0\n",
            "Episode: 91, Reward: -11.0\n",
            "Episode: 92, Reward: -15.0\n",
            "Episode: 93, Reward: -14.0\n",
            "Episode: 94, Reward: -9.0\n",
            "Episode: 95, Reward: -11.0\n",
            "Episode: 96, Reward: -9.0\n",
            "Episode: 97, Reward: -15.0\n",
            "Episode: 98, Reward: -13.0\n",
            "Episode: 99, Reward: -14.0\n",
            "Episode: 100, Reward: -13.0\n",
            "Episode: 101, Reward: -13.0\n",
            "Episode: 102, Reward: -11.0\n",
            "Episode: 103, Reward: -11.0\n",
            "Episode: 104, Reward: -9.0\n",
            "Episode: 105, Reward: -10.0\n",
            "Episode: 106, Reward: -15.0\n",
            "Episode: 107, Reward: -13.0\n",
            "Episode: 108, Reward: -14.0\n",
            "Episode: 109, Reward: -15.0\n",
            "Episode: 110, Reward: -10.0\n",
            "Episode: 111, Reward: -15.0\n",
            "Episode: 112, Reward: -14.0\n",
            "Episode: 113, Reward: -9.0\n",
            "Episode: 114, Reward: -9.0\n",
            "Episode: 115, Reward: -14.0\n",
            "Episode: 116, Reward: -9.0\n",
            "Episode: 117, Reward: -11.0\n",
            "Episode: 118, Reward: -14.0\n",
            "Episode: 119, Reward: -14.0\n",
            "Episode: 120, Reward: -11.0\n",
            "Episode: 121, Reward: -15.0\n",
            "Episode: 122, Reward: -14.0\n",
            "Episode: 123, Reward: -10.0\n",
            "Episode: 124, Reward: -11.0\n",
            "Episode: 125, Reward: -10.0\n",
            "Episode: 126, Reward: -10.0\n",
            "Episode: 127, Reward: -11.0\n",
            "Episode: 128, Reward: -14.0\n",
            "Episode: 129, Reward: -13.0\n",
            "Episode: 130, Reward: -15.0\n",
            "Episode: 131, Reward: -13.0\n",
            "Episode: 132, Reward: -10.0\n",
            "Episode: 133, Reward: -13.0\n",
            "Episode: 134, Reward: -9.0\n",
            "Episode: 135, Reward: -13.0\n",
            "Episode: 136, Reward: -9.0\n",
            "Episode: 137, Reward: -10.0\n",
            "Episode: 138, Reward: -13.0\n",
            "Episode: 139, Reward: -13.0\n",
            "Episode: 140, Reward: -13.0\n",
            "Episode: 141, Reward: -11.0\n",
            "Episode: 142, Reward: -11.0\n",
            "Episode: 143, Reward: -9.0\n",
            "Episode: 144, Reward: -9.0\n",
            "Episode: 145, Reward: -11.0\n",
            "Episode: 146, Reward: -13.0\n",
            "Episode: 147, Reward: -11.0\n",
            "Episode: 148, Reward: -15.0\n",
            "Episode: 149, Reward: -13.0\n",
            "Episode: 150, Reward: -13.0\n",
            "Episode: 151, Reward: -14.0\n",
            "Episode: 152, Reward: -15.0\n",
            "Episode: 153, Reward: -12.0\n",
            "Episode: 154, Reward: -15.0\n",
            "Episode: 155, Reward: -11.0\n",
            "Episode: 156, Reward: -10.0\n",
            "Episode: 157, Reward: -10.0\n",
            "Episode: 158, Reward: -11.0\n",
            "Episode: 159, Reward: -10.0\n",
            "Episode: 160, Reward: -11.0\n",
            "Episode: 161, Reward: -13.0\n",
            "Episode: 162, Reward: -10.0\n",
            "Episode: 163, Reward: -13.0\n",
            "Episode: 164, Reward: -15.0\n",
            "Episode: 165, Reward: -10.0\n",
            "Episode: 166, Reward: -11.0\n",
            "Episode: 167, Reward: -15.0\n",
            "Episode: 168, Reward: -11.0\n",
            "Episode: 169, Reward: -10.0\n",
            "Episode: 170, Reward: -9.0\n",
            "Episode: 171, Reward: -11.0\n",
            "Episode: 172, Reward: -10.0\n",
            "Episode: 173, Reward: -10.0\n",
            "Episode: 174, Reward: -11.0\n",
            "Episode: 175, Reward: -13.0\n",
            "Episode: 176, Reward: -13.0\n",
            "Episode: 177, Reward: -11.0\n",
            "Episode: 178, Reward: -15.0\n",
            "Episode: 179, Reward: -10.0\n",
            "Episode: 180, Reward: -10.0\n",
            "Episode: 181, Reward: -10.0\n",
            "Episode: 182, Reward: -11.0\n",
            "Episode: 183, Reward: -9.0\n",
            "Episode: 184, Reward: -13.0\n",
            "Episode: 185, Reward: -14.0\n",
            "Episode: 186, Reward: -11.0\n",
            "Episode: 187, Reward: -13.0\n",
            "Episode: 188, Reward: -9.0\n",
            "Episode: 189, Reward: -15.0\n",
            "Episode: 190, Reward: -14.0\n",
            "Episode: 191, Reward: -13.0\n",
            "Episode: 192, Reward: -11.0\n",
            "Episode: 193, Reward: -11.0\n",
            "Episode: 194, Reward: -9.0\n",
            "Episode: 195, Reward: -13.0\n",
            "Episode: 196, Reward: -10.0\n",
            "Episode: 197, Reward: -13.0\n",
            "Episode: 198, Reward: -14.0\n",
            "Episode: 199, Reward: -9.0\n",
            "Episode: 200, Reward: -15.0\n",
            "Episode: 201, Reward: -11.0\n",
            "Episode: 202, Reward: -10.0\n",
            "Episode: 203, Reward: -10.0\n",
            "Episode: 204, Reward: -9.0\n",
            "Episode: 205, Reward: -11.0\n",
            "Episode: 206, Reward: -13.0\n",
            "Episode: 207, Reward: -11.0\n",
            "Episode: 208, Reward: -11.0\n",
            "Episode: 209, Reward: -13.0\n",
            "Episode: 210, Reward: -13.0\n",
            "Episode: 211, Reward: -10.0\n",
            "Episode: 212, Reward: -13.0\n",
            "Episode: 213, Reward: -13.0\n",
            "Episode: 214, Reward: -15.0\n",
            "Episode: 215, Reward: -11.0\n",
            "Episode: 216, Reward: -13.0\n",
            "Episode: 217, Reward: -9.0\n",
            "Episode: 218, Reward: -11.0\n",
            "Episode: 219, Reward: -11.0\n",
            "Episode: 220, Reward: -13.0\n",
            "Episode: 221, Reward: -10.0\n",
            "Episode: 222, Reward: -13.0\n",
            "Episode: 223, Reward: -9.0\n",
            "Episode: 224, Reward: -11.0\n",
            "Episode: 225, Reward: -13.0\n",
            "Episode: 226, Reward: -13.0\n",
            "Episode: 227, Reward: -9.0\n",
            "Episode: 228, Reward: -11.0\n",
            "Episode: 229, Reward: -14.0\n"
          ]
        }
      ]
    }
  ]
}
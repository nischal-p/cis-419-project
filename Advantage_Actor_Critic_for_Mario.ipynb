{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advantage Actor Critic for Mario.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1QKnihasUHT-ODEJtZs9XP_Pwd90MSDV8",
      "authorship_tag": "ABX9TyP8mO5kubporM2a/N1q/8JL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nischal-p/cis-419-project/blob/main/Advantage_Actor_Critic_for_Mario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kymJzZuo6NOz"
      },
      "source": [
        "# Cartpole learning with Actor Critic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaAGd4N18vXf"
      },
      "source": [
        "## Installing dependancies for the display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJGB5ySZ82s_",
        "outputId": "54418584-c4d7-41f6-e6a5-daa0edb88945"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install gym-super-mario-bros==7.3.0\n",
        "!sudo apt-get install fceux"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym-super-mario-bros==7.3.0 in /usr/local/lib/python3.7/dist-packages (7.3.0)\n",
            "Requirement already satisfied: nes-py>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from gym-super-mario-bros==7.3.0) (8.1.8)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (4.62.3)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.0)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fceux is already the newest version (2.2.2+dfsg0-1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVEPqChcDGXD",
        "outputId": "d9530b84-977c-4a5f-e5f4-3ab31006d6bc"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay==0.2.*) (0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGePTIPo6QC-"
      },
      "source": [
        "## Importing Stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8_rmXAG6Fj6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "from itertools import count\n",
        "import os\n",
        "\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfZ2Fd176XO0"
      },
      "source": [
        "## Setting Up stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmtvdYqE9GTI",
        "outputId": "d7f3c25f-242a-4362-dc0d-48cf81271a72"
      },
      "source": [
        "# setting up the display\n",
        "import pyvirtualdisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "!echo $DISPLAY"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":1049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FKt9tPMxFtN",
        "outputId": "cdfc9189-4761-400f-bbbc-c099ae7f225c"
      },
      "source": [
        "# setting up environment and paramenters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = gym.make(\"SuperMarioBros-1-1-v0\")\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3),\n",
            " 0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB6hXufezq-j"
      },
      "source": [
        "# Pre-process Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKhy_dwhzuoV"
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = FrameStack(env, num_stack=4)\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "state_shape = env.observation_space.shape"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGK2LkA96c1w"
      },
      "source": [
        "## The Actor Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqpZzkI6fRY"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        c, h, w = self.input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.linear1 = nn.Linear(3136, 512)\n",
        "        self.linear2 = nn.Linear(512, self.output_dim)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, state):\n",
        "        output = F.relu(self.conv1(state))\n",
        "        output = F.relu(self.conv2(output))\n",
        "        output = F.relu(self.conv3(output))\n",
        "        output = self.flatten(output)\n",
        "        output = F.relu(self.linear1(output))\n",
        "        output = self.linear2(output)\n",
        "        distribution = Categorical(F.softmax(output, dim=-1))\n",
        "        return distribution"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmjoMz706icq"
      },
      "source": [
        "## The Critic Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95a7RbRy6nC8"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        c, h, w = self.input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.linear1 = nn.Linear(3136, 512)\n",
        "        self.linear2 = nn.Linear(512, 1)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, state):\n",
        "      output = F.relu(self.conv1(state))\n",
        "      output = F.relu(self.conv2(output))\n",
        "      output = F.relu(self.conv3(output))\n",
        "      output = self.flatten(output)\n",
        "      output = F.relu(self.linear1(output))\n",
        "      value = self.linear2(output)\n",
        "      return value"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxxjNC0c61Fn"
      },
      "source": [
        "## Compute returns \n",
        "\n",
        "It takes in rewards and computes the return for a certain episode/state using the discount factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9T872Wj7B1Y"
      },
      "source": [
        "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
        "    R = next_value\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        R = rewards[step] + gamma * R * masks[step]\n",
        "        returns.insert(0, R)\n",
        "    return returns\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olWBVGEiQoVs"
      },
      "source": [
        "# Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_1iv9wdQqwT"
      },
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "\n",
        "    def record(self, episode):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_rewards\", \"ep_lengths\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyoh7GaM7Z9x"
      },
      "source": [
        "# Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1frcB9wuOAKc"
      },
      "source": [
        "## Initializing display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2pWd9tTOC6c",
        "outputId": "369f7747-6a33-4c4b-fcc2-de7d702d914a"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "!echo $DISPLAY"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":1053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-25HnJPPON4-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7KAROqIK9X-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4963e076-5dac-40d6-fad5-46aeb57aee6b"
      },
      "source": [
        "save_dir = Path(\"mario_a2c_checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "actor = Actor(input_dim=(4, 84, 84), output_dim=env.action_space.n).to(device)\n",
        "critic = Critic(input_dim=(4, 84, 84), output_dim=env.action_space.n).to(device)\n",
        "\n",
        "episodes = 500\n",
        "\n",
        "save_every = 100\n",
        "log_every = 20\n",
        "\n",
        "optimizerA = optim.Adam(actor.parameters(), lr=0.00025)\n",
        "optimizerC = optim.Adam(critic.parameters(), lr=0.00025)\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    log_probs = []\n",
        "    values = []\n",
        "    rewards = []\n",
        "    masks = []\n",
        "    entropy = 0\n",
        "\n",
        "    # img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "    episode_rewards = 0\n",
        "\n",
        "    for i in count():\n",
        "\n",
        "        # Added\n",
        "        # img.set_data(env.render(mode='rgb_array'))\n",
        "        # plt.axis('off')\n",
        "        # display.display(plt.gcf())\n",
        "        # display.clear_output(wait=True)\n",
        "\n",
        "        state = state.__array__()\n",
        "        state = torch.tensor(state).to(device)\n",
        "        state = state.unsqueeze(0)\n",
        "        dist = actor(state)\n",
        "        value = critic(state)\n",
        "\n",
        "        action = dist.sample()\n",
        "\n",
        "        next_state, reward, done, info = env.step(int(action))\n",
        "\n",
        "        logger.log_step(reward)\n",
        "        episode_rewards += reward\n",
        "\n",
        "        log_prob = dist.log_prob(action).unsqueeze(0)\n",
        "        entropy += dist.entropy().mean()\n",
        "\n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
        "        masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done or info[\"flag_get\"]:\n",
        "            print(f'Episode: {e}, Reward: {episode_rewards}')\n",
        "            break\n",
        "    \n",
        "    logger.log_episode()\n",
        "\n",
        "    next_state = next_state.__array__()\n",
        "    next_state = torch.tensor(next_state).to(device)\n",
        "    next_state = next_state.unsqueeze(0)\n",
        "\n",
        "    next_value = critic(next_state)\n",
        "    returns = compute_returns(next_value, rewards, masks)\n",
        "\n",
        "    log_probs = torch.cat(log_probs)\n",
        "    returns = torch.cat(returns).detach()\n",
        "    values = torch.cat(values)\n",
        "\n",
        "    advantage = returns - values\n",
        "\n",
        "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "    critic_loss = advantage.pow(2).mean()\n",
        "\n",
        "    optimizerA.zero_grad()\n",
        "    optimizerC.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    critic_loss.backward()\n",
        "    optimizerA.step()\n",
        "    optimizerC.step()\n",
        "\n",
        "    # if e % log_every == 0:\n",
        "    #     logger.record(episode=e)\n",
        "\n",
        "    if e % save_every == 0:\n",
        "        torch.save(actor, save_dir / f'actor_{e}.pkl')\n",
        "        torch.save(critic, save_dir / f'critic{e}.pkl')\n",
        "# ipythondisplay.clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Reward: 231.0\n",
            "Episode: 1, Reward: 751.0\n",
            "Episode: 2, Reward: 231.0\n",
            "Episode: 3, Reward: 231.0\n",
            "Episode: 4, Reward: 231.0\n",
            "Episode: 5, Reward: 635.0\n",
            "Episode: 6, Reward: 606.0\n",
            "Episode: 7, Reward: 587.0\n",
            "Episode: 8, Reward: 231.0\n",
            "Episode: 9, Reward: 705.0\n",
            "Episode: 10, Reward: 588.0\n",
            "Episode: 11, Reward: 231.0\n",
            "Episode: 12, Reward: 231.0\n",
            "Episode: 13, Reward: 1175.0\n",
            "Episode: 14, Reward: 231.0\n",
            "Episode: 15, Reward: 627.0\n",
            "Episode: 16, Reward: 620.0\n",
            "Episode: 17, Reward: 708.0\n"
          ]
        }
      ]
    }
  ]
}